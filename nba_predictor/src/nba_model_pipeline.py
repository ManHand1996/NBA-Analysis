# src/nba_model_pipeline.py
"""
NBAæ¨¡åž‹ç®¡é“ - åè°ƒè®­ç»ƒå™¨å’Œç‰ˆæœ¬ç®¡ç†å™¨
"""
import pandas as pd
import pickle
import json
import os
from datetime import datetime
from src.trainer import MultiModelTrainer
from src.versioner import MultiModelVersioner
from src.reporter import generate_experiment_report


class NBAModelPipeline:
    """
    NBAæ¨¡åž‹ç®¡é“ - åè°ƒå™¨
    
    èŒè´£ï¼š
    1. åè°ƒMultiModelTrainerå’ŒMultiModelVersioner
    2. ç®¡ç†å®Œæ•´çš„è®­ç»ƒå’Œç‰ˆæœ¬ç®¡ç†æµç¨‹
    3. æä¾›ç»Ÿä¸€çš„é¢„æµ‹æŽ¥å£
    """
    
    def __init__(self, use_time_split=True, use_scaler=True):
        """
        åˆå§‹åŒ–ç®¡é“
        
        Args:
            use_time_split: æ˜¯å¦ä½¿ç”¨æ—¶é—´æ„ŸçŸ¥åˆ’åˆ†
            use_scaler: æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–
        """
        self.trainer = MultiModelTrainer(use_time_split, use_scaler)
        self.versioner = MultiModelVersioner("nba_experiments")
        self.current_experiment = None
        self.current_data = None  # ä¿å­˜å½“å‰æ•°æ®
        
        print(f"ðŸš€ NBAæ¨¡åž‹ç®¡é“å·²åˆå§‹åŒ–")
        print(f"   æ—¶é—´åˆ’åˆ†: {use_time_split}")
        print(f"   æ ‡å‡†åŒ–: {use_scaler}")
    

    def make_sklearn_compatible(self, model):
        """ä½¿æ¨¡åž‹å…¼å®¹ scikit-learn"""
        if not hasattr(model, '_estimator_type'):
            model._estimator_type = "classifier"
        return model


    def run_experiment(self, experiment_name, df, test_seasons=3):
        """
        è¿è¡Œå®Œæ•´å®žéªŒ
        
        Args:
            experiment_name: å®žéªŒåç§°
            df: åŒ…å«æ‰€æœ‰æ•°æ®çš„DataFrame
            test_seasons: æµ‹è¯•èµ›å­£æ•°é‡
            
        Returns:
            å®žéªŒç»“æžœå­—å…¸
        """
        print(f"\n{'='*60}")
        print(f"å¼€å§‹å®žéªŒ: {experiment_name}")
        print(f"{'='*60}")
        
        # 1. ä½¿ç”¨trainerå‡†å¤‡ç‰¹å¾
        print("\n1ï¸âƒ£ å‡†å¤‡ç‰¹å¾æ•°æ®...")
        X_train, X_test, y_train, y_test, feature_names, scaler = \
            self.trainer.prepare_features(df, test_seasons)
        
        # ä¿å­˜å½“å‰æ•°æ®ä¾›åŽç»­ä½¿ç”¨
        self.current_data = {
            'X_train': X_train, 'X_test': X_test,
            'y_train': y_train, 'y_test': y_test,
            'feature_names': feature_names,
            'scaler': scaler
        }
        
        # 2. åˆ›å»ºå®žéªŒï¼ˆé€šè¿‡versionerï¼‰
        print("\n2ï¸âƒ£ åˆ›å»ºå®žéªŒè®°å½•...")
        data_info = {
            'experiment_name': experiment_name,
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'feature_count': len(feature_names),
            'train_pos_ratio': y_train.mean(),
            'test_pos_ratio': y_test.mean(),
            'use_time_split': self.trainer.use_time_split,
            'use_scaler': self.trainer.use_scaler,
            'test_seasons': test_seasons
        }
        
        feature_info = {
            'feature_names': feature_names.tolist() if hasattr(feature_names, 'tolist') else list(feature_names),
            'scaler_used': scaler is not None
        }
        
        self.current_experiment, exp_dir = self.versioner.create_experiment(
            experiment_name, data_info, feature_info
        )
        
        print(f"   å®žéªŒID: {self.current_experiment}")
        
        # 3. è®­ç»ƒå¹¶ä¿å­˜æ‰€æœ‰æ¨¡åž‹
        print("\n3ï¸âƒ£ è®­ç»ƒæ‰€æœ‰æ¨¡åž‹...")
        models = ['xgboost', 'random_forest', 'gradient_boosting']
        results = {}
        
        for model_type in models:
            print(f"\n   ðŸ”„ è®­ç»ƒ{model_type}...")
            
            # è®­ç»ƒæ¨¡åž‹
            if model_type == 'xgboost':
                model = self.trainer.train_xgboost(X_train, y_train)
            elif model_type == 'random_forest':
                model = self.trainer.train_random_forest(X_train, y_train)
            else:  # gradient_boosting
                 # éœ€è¦å¡«å……NaN
                X_train = X_train.fillna(0)
                X_test = X_test.fillna(0)
                model = self.trainer.train_gradient_boosting(X_train, y_train)
            
            model = self.make_sklearn_compatible(model)

            # è¯„ä¼°æ¨¡åž‹
            metrics = self.trainer.evaluate_model(model, X_test, y_test)
            print(f"      å‡†ç¡®çŽ‡: {metrics['accuracy']:.2%}")
            
            # ä¿å­˜åˆ°ç‰ˆæœ¬ç®¡ç†å™¨
            model_config = self._get_default_model_config(model_type)
            
            result = self.versioner.save_model_result(
                experiment_id=self.current_experiment,
                model_type=model_type,
                model=model,
                X_train=X_train,
                y_train=y_train,
                X_test=X_test,
                y_test=y_test,
                model_config=model_config,
                scaler=scaler,
                notes=f"{model_type} with time_split={self.trainer.use_time_split}"
            )
            
            results[model_type] = result
        
        # 4. æ¯”è¾ƒæ¨¡åž‹
        print("\n4ï¸âƒ£ æ¯”è¾ƒæ¨¡åž‹æ€§èƒ½...")
        comparison = self.versioner.compare_models_in_experiment(self.current_experiment)
        
        # 5. èŽ·å–æœ€ä½³æ¨¡åž‹
        print("\n5ï¸âƒ£ ç¡®å®šæœ€ä½³æ¨¡åž‹...")
        best_model_info = self.versioner.get_best_model(self.current_experiment)
        
        if best_model_info:
            best_metrics = best_model_info['metadata']['metrics']
            print(f"   ðŸ† æœ€ä½³æ¨¡åž‹: {best_model_info['metadata']['model_type']}")
            print(f"       å‡†ç¡®çŽ‡: {best_metrics['accuracy']:.2%}")
            if 'auc' in best_metrics:
                print(f"       AUC: {best_metrics['auc']:.3f}")
        
        # 6. ä¿å­˜å®Œæ•´ç®¡é“
        print("\n6ï¸âƒ£ ä¿å­˜å®Œæ•´é¢„æµ‹ç®¡é“...")
        pipeline_path = self._save_complete_pipeline(best_model_info, scaler, feature_names)
        
        # 7. ç”Ÿæˆå®žéªŒæŠ¥å‘Š
        print("\n7ï¸âƒ£ ç”Ÿæˆå®žéªŒæŠ¥å‘Š...")
        self._generate_experiment_report(results, comparison, best_model_info)
        
        print(f"\n{'='*60}")
        print(f"å®žéªŒå®Œæˆ!")
        print(f"å®žéªŒID: {self.current_experiment}")
        print(f"æŠ¥å‘Šè·¯å¾„: {exp_dir}/experiment_report.md")
        print(f"ç®¡é“æ–‡ä»¶: {pipeline_path}")
        print(f"{'='*60}")
        
        return {
            'experiment_id': self.current_experiment,
            'results': results,
            'comparison': comparison,
            'best_model': best_model_info,
            'pipeline_path': pipeline_path
        }
    
    def _get_default_model_config(self, model_type):
        """èŽ·å–é»˜è®¤æ¨¡åž‹é…ç½®"""
        configs = {
            'xgboost': {
                'n_estimators': 200,
                'max_depth': 6,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42
            },
            'random_forest': {
                'n_estimators': 200,
                'max_depth': 10,
                'min_samples_split': 10,
                'min_samples_leaf': 5,
                'random_state': 42,
                'n_jobs': -1
            },
            'gradient_boosting': {
                'n_estimators': 200,
                'max_depth': 5,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'random_state': 42
            }
        }
        
        return configs.get(model_type, {})
    
    def _save_complete_pipeline(self, best_model_info, scaler, feature_names):
        """ä¿å­˜å®Œæ•´é¢„æµ‹ç®¡é“"""
        if not best_model_info:
            print("è­¦å‘Š: æ²¡æœ‰æœ€ä½³æ¨¡åž‹ä¿¡æ¯ï¼Œè·³è¿‡ç®¡é“ä¿å­˜")
            return None
        
        pipeline_package = {
            'model': best_model_info['model'],
            'scaler': scaler,
            'feature_names': feature_names,
            'experiment_id': self.current_experiment,
            'trainer_config': {
                'use_time_split': self.trainer.use_time_split,
                'use_scaler': self.trainer.use_scaler
            },
            'metadata': best_model_info['metadata'],
            'created_at': datetime.now().isoformat()
        }
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        
        os.makedirs("pipelines", exist_ok=True)
        
        pipeline_path = f"pipelines/{self.current_experiment}_pipeline.pkl"
        
        with open(pipeline_path, 'wb') as f:
            pickle.dump(pipeline_package, f)
        
        print(f"   âœ… ç®¡é“å·²ä¿å­˜: {pipeline_path}")
        
        return pipeline_path
    
    def _generate_experiment_report(self, results, comparison, best_model_info):
        """ç”Ÿæˆå®žéªŒæŠ¥å‘Š"""
        try:
            
            
            # å‡†å¤‡æŠ¥å‘Šæ•°æ®
            report_data = {
                'experiment_id': self.current_experiment,
                'results': results,
                'best_model': best_model_info
            }
            
            # è°ƒç”¨æŠ¥å‘Šç”Ÿæˆå™¨
            report_path = generate_experiment_report(report_data)
            return report_path
            
        except ImportError:
            print("è­¦å‘Š: æŠ¥å‘Šç”Ÿæˆå™¨æœªæ‰¾åˆ°ï¼Œè·³è¿‡æŠ¥å‘Šç”Ÿæˆ")
            return None
    
    def load_pipeline(self, experiment_id=None, pipeline_path=None):
        """
        åŠ è½½é¢„æµ‹ç®¡é“
        
        Args:
            experiment_id: å®žéªŒIDï¼Œå¦‚æžœæä¾›åˆ™åŠ è½½è¯¥å®žéªŒçš„æœ€ä½³æ¨¡åž‹
            pipeline_path: ç›´æŽ¥æŒ‡å®šç®¡é“æ–‡ä»¶è·¯å¾„
            
        Returns:
            åŠ è½½çš„ç®¡é“å¯¹è±¡
        """
        if pipeline_path:
            # ç›´æŽ¥åŠ è½½æŒ‡å®šè·¯å¾„
            try:
                with open(pipeline_path, 'rb') as f:
                    pipeline = pickle.load(f)
                
                print(f"âœ… ç®¡é“å·²åŠ è½½: {pipeline_path}")
                return pipeline
                
            except Exception as e:
                print(f"âŒ åŠ è½½ç®¡é“å¤±è´¥: {e}")
                return None
        
        elif experiment_id:
            # åŠ è½½æŒ‡å®šå®žéªŒçš„æœ€ä½³æ¨¡åž‹
            best_model_info = self.versioner.get_best_model(experiment_id)
            
            if not best_model_info:
                print(f"âŒ å®žéªŒ {experiment_id} æ²¡æœ‰æ‰¾åˆ°æœ€ä½³æ¨¡åž‹")
                return None
            
            # æž„å»ºç®¡é“åŒ…
            model_dir = best_model_info['model_dir']
            
            # å°è¯•åŠ è½½scaler
            scaler_path = model_dir / "scaler.pkl"
            if scaler_path.exists():
                with open(scaler_path, 'rb') as f:
                    scaler = pickle.load(f)
            else:
                scaler = None
            
            # åŠ è½½ç‰¹å¾åç§°ï¼ˆä»Žå®žéªŒé…ç½®ï¼‰
            exp_dir = self.versioner.base_dir / experiment_id
            feature_file = exp_dir / "features" / "feature_list.json"
            
            if feature_file.exists():
                with open(feature_file, 'r') as f:
                    feature_names = json.load(f)
            else:
                feature_names = None
            
            pipeline = {
                'model': best_model_info['model'],
                'scaler': scaler,
                'feature_names': feature_names,
                'experiment_id': experiment_id,
                'metadata': best_model_info['metadata']
            }
            
            print(f"âœ… å®žéªŒ {experiment_id} çš„ç®¡é“å·²æž„å»º")
            return pipeline
        
        else:
            print("âŒ å¿…é¡»æä¾›experiment_idæˆ–pipeline_path")
            return None
    
    def predict(self, new_features, experiment_id=None, pipeline_path=None):
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„ç®¡é“è¿›è¡Œé¢„æµ‹
        
        Args:
            new_features: æ–°æ•°æ®çš„ç‰¹å¾å­—å…¸
            experiment_id: å®žéªŒIDï¼ŒåŠ è½½è¯¥å®žéªŒçš„æœ€ä½³æ¨¡åž‹
            pipeline_path: ç›´æŽ¥æŒ‡å®šç®¡é“æ–‡ä»¶è·¯å¾„
            
        Returns:
            é¢„æµ‹ç»“æžœ
        """
        # 1. åŠ è½½ç®¡é“
        pipeline = self.load_pipeline(experiment_id, pipeline_path)
        
        if not pipeline:
            return None
        
        # 2. å‡†å¤‡ç‰¹å¾æ•°æ®
        feature_names = pipeline['feature_names']
        
        if feature_names is None:
            print("âŒ ç®¡é“ä¸­æ²¡æœ‰ç‰¹å¾åç§°ä¿¡æ¯")
            return None
        
        # åˆ›å»ºç‰¹å¾DataFrameï¼Œç¡®ä¿ç‰¹å¾é¡ºåºæ­£ç¡®
        feature_df = pd.DataFrame([new_features])
        
        # æ£€æŸ¥ç‰¹å¾æ˜¯å¦å®Œæ•´
        missing_features = set(feature_names) - set(feature_df.columns)
        if missing_features:
            print(f"è­¦å‘Š: ç¼ºå°‘ç‰¹å¾: {missing_features}")
            # å¡«å……ç¼ºå¤±ç‰¹å¾ä¸º0
            for feature in missing_features:
                feature_df[feature] = 0
        
        # ç¡®ä¿ç‰¹å¾é¡ºåº
        feature_df = feature_df[feature_names]
        
        # 3. æ ‡å‡†åŒ–å¤„ç†
        scaler = pipeline['scaler']
        if scaler is not None:
            try:
                feature_df_scaled = scaler.transform(feature_df)
            except Exception as e:
                print(f"âŒ æ ‡å‡†åŒ–å¤±è´¥: {e}")
                feature_df_scaled = feature_df.values
        else:
            feature_df_scaled = feature_df.values
        
        # 4. é¢„æµ‹
        model = pipeline['model']
        
        try:
            prediction = model.predict(feature_df_scaled)[0]
            
            result = {
                'prediction': int(prediction),
                'prediction_label': 'ä¸»èƒœ' if prediction == 1 else 'å®¢èƒœ'
            }
            
            # èŽ·å–æ¦‚çŽ‡ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
            if hasattr(model, 'predict_proba'):
                probability = model.predict_proba(feature_df_scaled)[0]
                result['probability'] = probability.tolist()
                result['win_probability'] = float(probability[1])
                result['confidence'] = self._get_confidence_level(probability[1])
            
            # æ·»åŠ æ¨¡åž‹ä¿¡æ¯
            metadata = pipeline.get('metadata', {})
            result['model_info'] = {
                'experiment_id': pipeline.get('experiment_id'),
                'model_type': metadata.get('model_type', type(model).__name__),
                'accuracy': metadata.get('metrics', {}).get('accuracy', 0)
            }
            
            print(f"âœ… é¢„æµ‹å®Œæˆ: {result['prediction_label']}")
            if 'win_probability' in result:
                print(f"   èŽ·èƒœæ¦‚çŽ‡: {result['win_probability']:.2%}")
            
            return result
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return None
    
    def _get_confidence_level(self, probability):
        """æ ¹æ®æ¦‚çŽ‡ç¡®å®šç½®ä¿¡åº¦"""
        if probability > 0.7 or probability < 0.3:
            return 'é«˜'
        elif probability > 0.6 or probability < 0.4:
            return 'ä¸­'
        else:
            return 'ä½Ž'
    
    def get_experiment_summary(self, experiment_id=None):
        """èŽ·å–å®žéªŒæ‘˜è¦"""
        if experiment_id is None:
            experiment_id = self.current_experiment
        
        if experiment_id is None:
            print("âŒ æ²¡æœ‰æŒ‡å®šå®žéªŒID")
            return None
        
        return self.versioner.compare_models_in_experiment(experiment_id)