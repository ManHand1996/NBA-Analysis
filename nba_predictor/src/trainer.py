# src/multi_model_trainer.py

import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import pandas as pd
import numpy as np

# src/multi_model_trainer.py
"""
çº¯ç²¹çš„å¤šæ¨¡å‹è®­ç»ƒå™¨ - ä¸“æ³¨äºç‰¹å¾å¤„ç†å’Œæ¨¡å‹è®­ç»ƒ
ä¸åŒ…å«ç‰ˆæœ¬ç®¡ç†åŠŸèƒ½ï¼Œç‰ˆæœ¬ç®¡ç†ç”±NBAModelPipelineè´Ÿè´£
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
import xgboost as xgb
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
import pickle

class MultiModelTrainer:
    """
    çº¯ç²¹çš„å¤šæ¨¡å‹è®­ç»ƒå™¨
    èŒè´£ï¼šç‰¹å¾å¤„ç†ã€æ•°æ®åˆ’åˆ†ã€æ¨¡å‹è®­ç»ƒ
    ä¸è´Ÿè´£ï¼šç‰ˆæœ¬ç®¡ç†ã€ç»“æœä¿å­˜ã€å®éªŒè®°å½•
    """
    
    def __init__(self, use_time_split=True, use_scaler=True):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        
        Args:
            use_time_split: æ˜¯å¦ä½¿ç”¨æ—¶é—´æ„ŸçŸ¥åˆ’åˆ†ï¼ˆTrue=æŒ‰èµ›å­£åˆ’åˆ†ï¼ŒFalse=éšæœºåˆ’åˆ†ï¼‰
            use_scaler: æ˜¯å¦ä½¿ç”¨æ ‡å‡†åŒ–
        """
        self.use_time_split = use_time_split
        self.use_scaler = use_scaler
        self.scaler = None
        self.feature_names = None
        self.data_info = {}
    
    def prepare_features(self, df, test_seasons=3):
        """
        å‡†å¤‡ç‰¹å¾å’Œæ ‡ç­¾ï¼Œæ”¯æŒä¸¤ç§åˆ’åˆ†æ–¹å¼
        
        Args:
            df: åŒ…å«ç‰¹å¾çš„DataFrameï¼Œå¿…é¡»æœ‰'season'åˆ—
            test_seasons: ç”¨æœ€åå‡ ä¸ªèµ›å­£ä½œä¸ºæµ‹è¯•é›†ï¼ˆä»…å½“use_time_split=Trueæ—¶æœ‰æ•ˆï¼‰
            
        Returns:
            X_train, X_test, y_train, y_test, feature_names, scaler
        """
        print(f"ğŸ”§ å‡†å¤‡ç‰¹å¾æ•°æ®...")
        print(f"   æ—¶é—´åˆ’åˆ†: {self.use_time_split}")
        print(f"   æ ‡å‡†åŒ–: {self.use_scaler}")
        
        # éªŒè¯å¿…éœ€çš„åˆ—
        required_columns = ['home_win', 'season']
        for col in required_columns:
            if col not in df.columns:
                raise ValueError(f"DataFrameå¿…é¡»åŒ…å«'{col}'åˆ—")
        
        # ç¡®å®šç‰¹å¾åˆ—ï¼ˆæ’é™¤æ ‡è¯†åˆ—å’Œæ ‡ç­¾åˆ—ï¼‰
        exclude_columns = [
            'home_team_id', 'away_team_id',
            'home_win', 'game_date', 'season', 'pts_diff'
            ]
        
        feature_names = [col for col in df.columns if col not in exclude_columns]
        self.feature_names = feature_names
        
        # åˆ’åˆ†æ•°æ®é›†
        if self.use_time_split:
            X_train, X_test, y_train, y_test = self._time_based_split(
                df, feature_names, test_seasons
            )
        else:
            X_train, X_test, y_train, y_test = self._random_split(
                df, feature_names
            )
        
        # æ ‡å‡†åŒ–å¤„ç†
        scaler = None
        if self.use_scaler:
            X_train, X_test, scaler = self._apply_scaling(X_train, X_test)
            self.scaler = scaler
        
        # è®°å½•æ•°æ®ä¿¡æ¯
        self.data_info = {
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'feature_count': len(feature_names),
            'train_pos_ratio': y_train.mean(),
            'test_pos_ratio': y_test.mean(),
            'use_time_split': self.use_time_split,
            'use_scaler': self.use_scaler
        }
        
        print(f"âœ… ç‰¹å¾å‡†å¤‡å®Œæˆ")
        print(f"   è®­ç»ƒé›†: {len(X_train)} æ ·æœ¬")
        print(f"   æµ‹è¯•é›†: {len(X_test)} æ ·æœ¬")
        print(f"   ç‰¹å¾æ•°: {len(feature_names)}")
        print(f"   æ ‡å‡†åŒ–å™¨: {'å·²åˆ›å»º' if scaler else 'æœªä½¿ç”¨'}")
        
        return X_train, X_test, y_train, y_test, feature_names, scaler
    
    def _time_based_split(self, df, feature_names, test_seasons):
        """æ—¶é—´æ„ŸçŸ¥åˆ’åˆ†ï¼ˆæ— æ•°æ®æ³„æ¼ï¼‰"""
        # æŒ‰èµ›å­£æ’åº
        seasons = sorted(df['season'].unique())
        
        if len(seasons) <= test_seasons:
            raise ValueError(f"èµ›å­£æ•°é‡({len(seasons)})å°‘äºæµ‹è¯•èµ›å­£æ•°({test_seasons})")
        
        test_season_cutoff = seasons[-test_seasons]
        
        # åˆ›å»ºæ©ç 
        train_mask = df['season'] < test_season_cutoff
        test_mask = df['season'] >= test_season_cutoff
        
        # æå–æ•°æ®
        X_train = df.loc[train_mask, feature_names]
        y_train = df.loc[train_mask, 'home_win']
        X_test = df.loc[test_mask, feature_names]
        y_test = df.loc[test_mask, 'home_win']
        
        print(f"   æ—¶é—´åˆ’åˆ†: è®­ç»ƒé›†({seasons[0]}-{test_season_cutoff-1}) "
              f"æµ‹è¯•é›†({test_season_cutoff}-{seasons[-1]})")
        
        return X_train, X_test, y_train, y_test
    
    def _random_split(self, df, feature_names):
        """éšæœºåˆ’åˆ†ï¼ˆç”¨äºå¿«é€ŸéªŒè¯ï¼‰"""
        X = df[feature_names]
        y = df['home_win']
        
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42, stratify=y
        )
        
        print(f"   éšæœºåˆ’åˆ†: è®­ç»ƒé›†{len(X_train)}æ ·æœ¬, æµ‹è¯•é›†{len(X_test)}æ ·æœ¬")
        
        return X_train, X_test, y_train, y_test
    
    def _apply_scaling(self, X_train, X_test):
        """åº”ç”¨æ ‡å‡†åŒ–"""
        # ç¡®å®šéœ€è¦æ ‡å‡†åŒ–çš„åˆ—ï¼ˆæ•°å€¼åˆ—ï¼Œæ’é™¤æ ‡è®°åˆ—ï¼‰
        numeric_cols = X_train.select_dtypes(include=[np.number]).columns
        exclude_cols = [col for col in numeric_cols 
                       if any(keyword in col for keyword in 
                             ['exists', 'is_', 'games_played', 'streak', 'count'])]
        
        scale_cols = [col for col in numeric_cols if col not in exclude_cols]
        
        if len(scale_cols) == 0:
            print("   è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°éœ€è¦æ ‡å‡†åŒ–çš„ç‰¹å¾")
            return X_train, X_test, None
        
        scaler = StandardScaler()
        
        # å¤åˆ¶æ•°æ®é¿å…è­¦å‘Š
        X_train_scaled = X_train.copy()
        X_test_scaled = X_test.copy()
        
        # æ‹Ÿåˆå¹¶è½¬æ¢è®­ç»ƒé›†
        X_train_scaled[scale_cols] = scaler.fit_transform(X_train[scale_cols])
        # è½¬æ¢æµ‹è¯•é›†ï¼ˆä½¿ç”¨è®­ç»ƒé›†çš„ç»Ÿè®¡é‡ï¼‰
        X_test_scaled[scale_cols] = scaler.transform(X_test[scale_cols])
        
        print(f"   æ ‡å‡†åŒ–: å¤„ç†äº†{len(scale_cols)}ä¸ªç‰¹å¾")
        
        return X_train_scaled, X_test_scaled, scaler
    
    def train_xgboost(self, X_train, y_train, params=None):
        """
        è®­ç»ƒXGBoostæ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            params: æ¨¡å‹å‚æ•°ï¼Œå¦‚æœä¸ºNoneä½¿ç”¨é»˜è®¤å‚æ•°
            
        Returns:
            è®­ç»ƒå¥½çš„XGBoostæ¨¡å‹
        """
        print(f"ğŸŒ² è®­ç»ƒXGBoostæ¨¡å‹...")
        
        if params is None:
            params = { 
                'n_estimators': 200,
                'max_depth': 6,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'colsample_bytree': 0.8,
                'random_state': 42,
                'eval_metric': 'logloss',
                'use_label_encoder': False
            }
        
        model = xgb.XGBClassifier(**params)
        model.fit(X_train, y_train)
        
        print(f"   âœ… XGBoostè®­ç»ƒå®Œæˆ")
        print(f"      å‚æ•°: n_estimators={params['n_estimators']}, "
              f"max_depth={params['max_depth']}, lr={params['learning_rate']}")
        
        return model
    
    def train_random_forest(self, X_train, y_train, params=None):
        """
        è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            params: æ¨¡å‹å‚æ•°ï¼Œå¦‚æœä¸ºNoneä½¿ç”¨é»˜è®¤å‚æ•°
            
        Returns:
            è®­ç»ƒå¥½çš„éšæœºæ£®æ—æ¨¡å‹
        """
        print(f"ğŸŒ³ è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
        
        if params is None:
            params = {
                'n_estimators': 200,
                'max_depth': 10,
                'min_samples_split': 10,
                'min_samples_leaf': 5,
                'random_state': 42,
                'n_jobs': -1,
                'class_weight': 'balanced'
            }
        
        model = RandomForestClassifier(**params)
        model.fit(X_train, y_train)
        
        print(f"   âœ… éšæœºæ£®æ—è®­ç»ƒå®Œæˆ")
        print(f"      å‚æ•°: n_estimators={params['n_estimators']}, "
              f"max_depth={params['max_depth']}")
        
        return model
    
    def train_gradient_boosting(self, X_train, y_train, params=None):
        """
        è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹
        
        Args:
            X_train: è®­ç»ƒç‰¹å¾
            y_train: è®­ç»ƒæ ‡ç­¾
            params: æ¨¡å‹å‚æ•°ï¼Œå¦‚æœä¸ºNoneä½¿ç”¨é»˜è®¤å‚æ•°
            
        Returns:
            è®­ç»ƒå¥½çš„æ¢¯åº¦æå‡æ¨¡å‹
        """
        print(f"ğŸ“ˆ è®­ç»ƒæ¢¯åº¦æå‡æ¨¡å‹...")
        
        if params is None:
            params = {
                'n_estimators': 200,
                'max_depth': 5,
                'learning_rate': 0.05,
                'subsample': 0.8,
                'random_state': 42
            }
        
        model = GradientBoostingClassifier(**params)
       

        model.fit(X_train, y_train)
        
        print(f"   âœ… æ¢¯åº¦æå‡è®­ç»ƒå®Œæˆ")
        print(f"      å‚æ•°: n_estimators={params['n_estimators']}, "
              f"max_depth={params['max_depth']}, lr={params['learning_rate']}")
        
        return model
    
    def evaluate_model(self, model, X_test, y_test):
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½
        
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            X_test: æµ‹è¯•ç‰¹å¾
            y_test: æµ‹è¯•æ ‡ç­¾
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        from sklearn.metrics import (accuracy_score, precision_score, 
                                   recall_score, f1_score, roc_auc_score)
        
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0)
        }
        
        if y_prob is not None:
            metrics['auc'] = roc_auc_score(y_test, y_prob)
        
        return metrics
    
    def get_feature_importance(self, model, feature_names=None):
        """
        è·å–ç‰¹å¾é‡è¦æ€§
        
        Args:
            model: è®­ç»ƒå¥½çš„æ¨¡å‹
            feature_names: ç‰¹å¾åç§°åˆ—è¡¨ï¼Œå¦‚æœä¸ºNoneä½¿ç”¨self.feature_names
            
        Returns:
            DataFrameåŒ…å«ç‰¹å¾å’Œé‡è¦æ€§åˆ†æ•°
        """
        if feature_names is None:
            feature_names = self.feature_names
        
        if hasattr(model, 'feature_importances_'):
            importance_df = pd.DataFrame({
                'feature': feature_names,
                'importance': model.feature_importances_
            }).sort_values('importance', ascending=False)
            
            return importance_df
        else:
            print("è­¦å‘Š: è¯¥æ¨¡å‹æ²¡æœ‰feature_importances_å±æ€§")
            return None
    
    def save_scaler(self, filepath):
        """ä¿å­˜æ ‡å‡†åŒ–å™¨"""
        if self.scaler is not None:
            with open(filepath, 'wb') as f:
                pickle.dump(self.scaler, f)
            print(f"âœ… æ ‡å‡†åŒ–å™¨å·²ä¿å­˜: {filepath}")
            return True
        else:
            print("è­¦å‘Š: æ²¡æœ‰æ ‡å‡†åŒ–å™¨å¯ä¿å­˜")
            return False
    
    def load_scaler(self, filepath):
        """åŠ è½½æ ‡å‡†åŒ–å™¨"""
        try:
            with open(filepath, 'rb') as f:
                self.scaler = pickle.load(f)
            print(f"âœ… æ ‡å‡†åŒ–å™¨å·²åŠ è½½: {filepath}")
            return True
        except Exception as e:
            print(f"âŒ åŠ è½½æ ‡å‡†åŒ–å™¨å¤±è´¥: {e}")
            return False
    
    def save_model(self, model, filepath):
        """ä¿å­˜æ¨¡å‹"""
        try:
            with open(filepath, 'wb') as f:
                pickle.dump(model, f)
            print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {filepath}")
            return True
        except Exception as e:
            print(f"âŒ ä¿å­˜æ¨¡å‹å¤±è´¥: {e}")
            return False
    
    def load_model(self, filepath):
        """åŠ è½½æ¨¡å‹"""
        try:
            with open(filepath, 'rb') as f:
                model = pickle.load(f)
            print(f"âœ… æ¨¡å‹å·²åŠ è½½: {filepath}")
            return model
        except Exception as e:
            print(f"âŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            return None

