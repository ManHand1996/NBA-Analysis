# src/multi_model_versioner.py
import os
import json
import pickle
import yaml
import pandas as pd
import numpy as np
from datetime import datetime
from pathlib import Path
import shutil
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import (accuracy_score, roc_auc_score, 
                           confusion_matrix, roc_curve, auc)

class MultiModelVersioner:
    """æ”¯æŒå¤šæ¨¡å‹å®éªŒçš„ç‰ˆæœ¬ç®¡ç†å™¨"""
    
    def __init__(self, base_dir="model_experiments"):
        self.base_dir = Path(base_dir)
        self.base_dir.mkdir(exist_ok=True)
        self.save_molde_name = 'model.pkl'
        
        # åˆå§‹åŒ–ç´¢å¼•æ–‡ä»¶
        self.index_file = self.base_dir / "experiments_index.csv"
        if not self.index_file.exists():
            pd.DataFrame(columns=[
                'experiment_id', 'version', 'model_type', 
                'accuracy', 'auc', 'best_model', 'created_at', 'notes'
            ]).to_csv(self.index_file, index=False)
    
    def create_experiment(self, experiment_name, data_info, feature_info):
        """
        åˆ›å»ºä¸€ä¸ªæ–°çš„å®éªŒ
        
        Args:
            experiment_name: å®éªŒåç§°ï¼Œå¦‚ "playoff_feature_test"
            data_info: æ•°æ®ä¿¡æ¯å­—å…¸
            feature_info: ç‰¹å¾ä¿¡æ¯å­—å…¸
        """
        # åˆ›å»ºå®éªŒç›®å½•
        experiment_id = self._generate_experiment_id()
        exp_dir = self.base_dir / experiment_id
        exp_dir.mkdir(parents=True)
        
        # åˆ›å»ºå­ç›®å½•ç»“æ„
        (exp_dir / "models").mkdir()
        (exp_dir / "results").mkdir()
        # (exp_dir / "features").mkdir()
        # (exp_dir / "configs").mkdir()
        # (exp_dir / "artifacts").mkdir()
        
        # ä¿å­˜å®éªŒé…ç½®
        experiment_config = {
            'experiment_id': experiment_id,
            'experiment_name': experiment_name,
            'created_at': datetime.now().isoformat(),
            'data_info': data_info,
            'feature_info': feature_info,
            'models_tested': [],
            'best_model': None,
            'status': 'running'
        }
        
        with open(exp_dir / "experiment_config.json", 'w') as f:
            json.dump(experiment_config, f, indent=2)
        
        print(f"âœ… å®éªŒ {experiment_name} å·²åˆ›å»º")
        print(f"   å®éªŒID: {experiment_id}")
        print(f"   è·¯å¾„: {exp_dir}")
        
        return experiment_id, exp_dir
    
    def save_model_result(self, experiment_id, model_type, model, 
                         X_train, y_train, X_test, y_test, 
                         model_config, scaler=None, notes=""):
        """
        ä¿å­˜å•ä¸ªæ¨¡å‹çš„è®­ç»ƒç»“æœ
        
        Args:
            experiment_id: å®éªŒID
            model_type: æ¨¡å‹ç±»å‹ï¼Œå¦‚ 'xgboost', 'random_forest'
            model: è®­ç»ƒå¥½çš„æ¨¡å‹å¯¹è±¡
            X_train, y_train: è®­ç»ƒæ•°æ®
            X_test, y_test: æµ‹è¯•æ•°æ®
            model_config: æ¨¡å‹é…ç½®å‚æ•°
            notes: æ¨¡å‹è¯´æ˜
        """
        exp_dir = self.base_dir / experiment_id
        
        # ä¸ºè¿™ä¸ªæ¨¡å‹åˆ›å»ºç‰ˆæœ¬
        model_version = self._get_next_model_version(exp_dir, model_type)
        model_dir = exp_dir / "models" / model_type / f"v{model_version}"
        model_dir.mkdir(parents=True)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        metrics = self._compute_model_metrics(model, X_test, y_test)
        
        # ä¿å­˜æ¨¡å‹
        # self._save_model_files(model, model_dir, model_type, model_version)
        self._save_model_files(model, model_dir, model_type, model_version, scaler)
        # ä¿å­˜æ¨¡å‹å…ƒæ•°æ®
        metadata = {
            'experiment_id': experiment_id,
            'model_type': model_type,
            'model_version': model_version,
            'created_at': datetime.now().isoformat(),
            'metrics': metrics,
            'model_config': model_config,
            'notes': notes,
            'data_info': {
                'train_samples': len(X_train),
                'test_samples': len(X_test),
                'feature_count': X_train.shape[1],
                'train_pos_ratio': y_train.mean(),
                'test_pos_ratio': y_test.mean()
            }
        }
        
        with open(model_dir / "metadata.json", 'w') as f:
            json.dump(metadata, f, indent=2)
        
        # ä¿å­˜ç‰¹å¾é‡è¦æ€§
        if hasattr(model, 'feature_importances_'):
            self._save_feature_importance(model, X_train.columns, model_dir)
        
        # ç”Ÿæˆå¯è§†åŒ–ç»“æœ
        self._generate_model_artifacts(model, X_test, y_test, model_dir)
        
        # æ›´æ–°å®éªŒé…ç½®
        self._update_experiment_config(exp_dir, model_type, model_version, metrics)
        
        # æ›´æ–°å…¨å±€ç´¢å¼•
        self._update_experiments_index(experiment_id, model_type, model_version, metrics, notes)
        
        print(f"  âœ… {model_type} v{model_version} å·²ä¿å­˜")
        print(f"     å‡†ç¡®ç‡: {metrics['accuracy']:.2%}, AUC: {metrics.get('auc', 'N/A')}")
        
        return {
            'model_type': model_type,
            'model_version': model_version,
            'metrics': metrics,
            'model_dir': model_dir
        }
    
    def compare_models_in_experiment(self, experiment_id):
        """æ¯”è¾ƒå®éªŒä¸­çš„æ‰€æœ‰æ¨¡å‹"""
        exp_dir = self.base_dir / experiment_id
        
        if not exp_dir.exists():
            raise ValueError(f"å®éªŒ {experiment_id} ä¸å­˜åœ¨")
        
        # åŠ è½½æ‰€æœ‰æ¨¡å‹çš„å…ƒæ•°æ®
        models_data = []
        models_dir = exp_dir / "models"
        
        for model_type in models_dir.iterdir():
            if model_type.is_dir():
                for model_version in model_type.iterdir():
                    if model_version.is_dir():
                        meta_file = model_version / "metadata.json"
                        if meta_file.exists():
                            with open(meta_file, 'r') as f:
                                metadata = json.load(f)
                            models_data.append(metadata)
        
        if not models_data:
            print("å®éªŒä¸­æ²¡æœ‰æ‰¾åˆ°æ¨¡å‹æ•°æ®")
            return None
        
        # åˆ›å»ºæ¯”è¾ƒè¡¨æ ¼
        comparison_df = pd.DataFrame([
            {
                'Model': f"{d['model_type']} v{d['model_version']}",
                'Accuracy': d['metrics']['accuracy'],
                'AUC': d['metrics'].get('auc', 0),
                'Precision': d['metrics']['precision'],
                'Recall': d['metrics']['recall'],
                'F1': d['metrics']['f1_score'],
                'Train Samples': d['data_info']['train_samples'],
                'Features': d['data_info']['feature_count'],
                'Config': str(d['model_config'])[:50] + '...'
            }
            for d in models_data
        ])
        
        # æ’åºå¹¶æ˜¾ç¤º
        comparison_df = comparison_df.sort_values('Accuracy', ascending=False)
        
        print("\n" + "="*100)
        print(f"å®éªŒ {experiment_id} - æ¨¡å‹æ¯”è¾ƒ")
        print("="*100)
        print(comparison_df.to_string(index=False))
        
        # ç”Ÿæˆæ¯”è¾ƒå›¾è¡¨
        self._plot_model_comparison(comparison_df, exp_dir)
        
        # ä¿å­˜æ¯”è¾ƒç»“æœ
        comparison_df.to_csv(exp_dir / "results" / "model_comparison.csv", index=False)
        
        # è¯†åˆ«æœ€ä½³æ¨¡å‹
        best_model = comparison_df.iloc[0]
        print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model['Model']}")
        print(f"   å‡†ç¡®ç‡: {best_model['Accuracy']:.2%}")
        print(f"   AUC: {best_model['AUC']:.3f}")
        
        return comparison_df
    
    def get_best_model(self, experiment_id):
        """è·å–å®éªŒä¸­çš„æœ€ä½³æ¨¡å‹"""
        exp_dir = self.base_dir / experiment_id
        
        # åŠ è½½å®éªŒé…ç½®
        config_file = exp_dir / "experiment_config.json"
        if not config_file.exists():
            raise ValueError(f"å®éªŒé…ç½®ä¸å­˜åœ¨: {config_file}")
        
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        if not config.get('best_model'):
            print("å®éªŒå°šæœªå®Œæˆæˆ–æ²¡æœ‰æœ€ä½³æ¨¡å‹æ ‡è®°")
            return None
        
        best_model_info = config['best_model']
        model_type = best_model_info['model_type']
        model_version = best_model_info['model_version']
        
        # åŠ è½½æœ€ä½³æ¨¡å‹
        model_dir = exp_dir / "models" / model_type / f"v{model_version}"
        model_path = model_dir / "model.pkl"
        
        with open(model_path, 'rb') as f:
            model = pickle.load(f)
        
        # åŠ è½½å…ƒæ•°æ®
        meta_path = model_dir / "metadata.json"
        with open(meta_path, 'r') as f:
            metadata = json.load(f)
        
        print(f"ğŸ¯ æœ€ä½³æ¨¡å‹: {model_type} v{model_version}")
        print(f"   å‡†ç¡®ç‡: {metadata['metrics']['accuracy']:.2%}")
        print(f"   è·¯å¾„: {model_dir}")
        
        return {
            'model': model,
            'metadata': metadata,
            'model_dir': model_dir
        }
    
    def _generate_experiment_id(self):
        """ç”Ÿæˆå®éªŒIDï¼ˆæ—¶é—´æˆ³ï¼‰"""
        return datetime.now().strftime("exp_%Y%m%d_%H%M%S")
    
    def _get_next_model_version(self, exp_dir, model_type):
        """è·å–æ¨¡å‹çš„ä¸‹ä¸€ä¸ªç‰ˆæœ¬å·"""
        model_type_dir = exp_dir / "models" / model_type
        if not model_type_dir.exists():
            return 1
        
        versions = []
        for item in model_type_dir.iterdir():
            if item.is_dir() and item.name.startswith('v'):
                try:
                    version = int(item.name[1:])
                    versions.append(version)
                except:
                    pass
        
        return max(versions) + 1 if versions else 1
    
    def _compute_model_metrics(self, model, X_test, y_test):
        """è®¡ç®—æ¨¡å‹è¯„ä¼°æŒ‡æ ‡"""
        y_pred = model.predict(X_test)
        y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, 'predict_proba') else None
        
        from sklearn.metrics import precision_score, recall_score, f1_score
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1_score': f1_score(y_test, y_pred, zero_division=0)
        }
        
        if y_prob is not None:
            metrics['auc'] = roc_auc_score(y_test, y_prob)
        
        return metrics
    
    def _save_model_files(self, model, model_dir, model_type, model_version, scaler=None):
        """ä¿å­˜æ¨¡å‹æ–‡ä»¶"""

        import pickle
    
        # ä¿å­˜å®Œæ•´çš„æ¨¡å‹åŒ…
        model_package = {
            'model': model,
            'scaler': scaler,  # ä¿å­˜scaler
            'model_type': model_type,
            'model_version': model_version,
            'saved_at': datetime.now().isoformat()
        }
        
        with open(model_dir / self.save_molde_name, 'wb') as f:
            pickle.dump(model_package, f)
        
        # # å¯é€‰ï¼šå•ç‹¬ä¿å­˜æ¨¡å‹ï¼ˆå¦‚æœéœ€è¦ï¼‰
        # with open(model_dir / "model_only.pkl", 'wb') as f:
        #     pickle.dump(model, f)
        
        # # å•ç‹¬ä¿å­˜scaler
        # if scaler is not None:
        #     with open(model_dir / "scaler.pkl", 'wb') as f:
        #         pickle.dump(scaler, f)

        # å¦‚æœæ˜¯XGBoostï¼Œé¢å¤–ä¿å­˜åŸç”Ÿæ ¼å¼
        if model_type == 'xgboost' and hasattr(model, 'save_model'):
            model.save_model(model_dir / "model.json")
    
    def _save_feature_importance(self, model, feature_names, model_dir):
        """ä¿å­˜ç‰¹å¾é‡è¦æ€§"""
        importance_df = pd.DataFrame({
            'feature': feature_names,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        importance_df.to_csv(model_dir / "feature_importance.csv", index=False)
        
        # ä¿å­˜å‰20ä¸ªç‰¹å¾çš„å›¾è¡¨
        plt.figure(figsize=(10, 8))
        top_features = importance_df.head(20).sort_values('importance')
        plt.barh(range(len(top_features)), top_features['importance'])
        plt.yticks(range(len(top_features)), top_features['feature'])
        plt.xlabel('Importance')
        plt.title('Top 20 Feature Importance')
        plt.tight_layout()
        plt.savefig(model_dir / "feature_importance.png", dpi=150)
        plt.close()
    
    def _generate_model_artifacts(self, model, X_test, y_test, model_dir):
        """ç”Ÿæˆæ¨¡å‹å¯è§†åŒ–ç»“æœ"""
        artifacts_dir = model_dir / "artifacts"
        artifacts_dir.mkdir(exist_ok=True)
        
        try:
            # æ··æ·†çŸ©é˜µ
            y_pred = model.predict(X_test)
            cm = confusion_matrix(y_test, y_pred)
            
            plt.figure(figsize=(8, 6))
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
            plt.title('Confusion Matrix')
            plt.ylabel('True Label')
            plt.xlabel('Predicted Label')
            plt.tight_layout()
            plt.savefig(artifacts_dir / "confusion_matrix.png", dpi=150)
            plt.close()
            
            # ROCæ›²çº¿
            if hasattr(model, 'predict_proba'):
                y_prob = model.predict_proba(X_test)[:, 1]
                fpr, tpr, _ = roc_curve(y_test, y_prob)
                roc_auc = auc(fpr, tpr)
                
                plt.figure(figsize=(8, 6))
                plt.plot(fpr, tpr, color='darkorange', lw=2, 
                        label=f'ROC curve (AUC = {roc_auc:.3f})')
                plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
                plt.xlim([0.0, 1.0])
                plt.ylim([0.0, 1.05])
                plt.xlabel('False Positive Rate')
                plt.ylabel('True Positive Rate')
                plt.title('ROC Curve')
                plt.legend(loc="lower right")
                plt.tight_layout()
                plt.savefig(artifacts_dir / "roc_curve.png", dpi=150)
                plt.close()
        except Exception as e:
            print(f"è­¦å‘Š: ç”Ÿæˆå¯è§†åŒ–ç»“æœæ—¶å‡ºé”™: {e}")
    
    def _update_experiment_config(self, exp_dir, model_type, model_version, metrics):
        """æ›´æ–°å®éªŒé…ç½®"""
        config_file = exp_dir / "experiment_config.json"
        with open(config_file, 'r') as f:
            config = json.load(f)
        
        # æ·»åŠ æ¨¡å‹åˆ°å·²æµ‹è¯•åˆ—è¡¨
        model_info = {
            'model_type': model_type,
            'model_version': model_version,
            'accuracy': metrics['accuracy'],
            'auc': metrics.get('auc', 0)
        }
        
        if 'models_tested' not in config:
            config['models_tested'] = []
        
        config['models_tested'].append(model_info)
        
        # æ›´æ–°æœ€ä½³æ¨¡å‹
        if not config.get('best_model') or metrics['accuracy'] > config['best_model'].get('accuracy', 0):
            config['best_model'] = model_info
        
        # å¦‚æœæ‰€æœ‰æ¨¡å‹éƒ½æµ‹è¯•å®Œæˆï¼Œæ›´æ–°çŠ¶æ€
        if len(config['models_tested']) >= len(config.get('planned_models', [])):
            config['status'] = 'completed'
        
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
    
    def _update_experiments_index(self, experiment_id, model_type, model_version, metrics, notes):
        """æ›´æ–°å…¨å±€å®éªŒç´¢å¼•"""
        if self.index_file.exists():
            index_df = pd.read_csv(self.index_file)
        else:
            index_df = pd.DataFrame()
        
        new_entry = {
            'experiment_id': experiment_id,
            'version': model_version,
            'model_type': model_type,
            'accuracy': metrics['accuracy'],
            'auc': metrics.get('auc', 0),
            'best_model': False,  # ç¨åæ›´æ–°
            'created_at': datetime.now().strftime('%Y-%m-%d %H:%M'),
            'notes': notes[:100]
        }
        
        index_df = pd.concat([index_df, pd.DataFrame([new_entry])], ignore_index=True)
        index_df.to_csv(self.index_file, index=False)
    
    def _plot_model_comparison(self, comparison_df, exp_dir):
        """ç»˜åˆ¶æ¨¡å‹æ¯”è¾ƒå›¾"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 12))
        
        # å‡†ç¡®ç‡æ¯”è¾ƒ
        axes[0, 0].barh(comparison_df['Model'], comparison_df['Accuracy'] * 100)
        axes[0, 0].set_xlabel('Accuracy (%)')
        axes[0, 0].set_title('Model Accuracy Comparison')
        axes[0, 0].axvline(x=50, color='r', linestyle='--', alpha=0.5)
        
        # AUCæ¯”è¾ƒ
        if 'AUC' in comparison_df.columns and comparison_df['AUC'].notna().any():
            axes[0, 1].barh(comparison_df['Model'], comparison_df['AUC'])
            axes[0, 1].set_xlabel('AUC')
            axes[0, 1].set_title('Model AUC Comparison')
            axes[0, 1].axvline(x=0.5, color='r', linestyle='--', alpha=0.5)
        
        # F1-scoreæ¯”è¾ƒ
        axes[1, 0].barh(comparison_df['Model'], comparison_df['F1'])
        axes[1, 0].set_xlabel('F1 Score')
        axes[1, 0].set_title('Model F1-Score Comparison')
        
        # ç²¾åº¦-å¬å›ç‡æ•£ç‚¹å›¾
        axes[1, 1].scatter(comparison_df['Precision'], comparison_df['Recall'], s=100)
        for idx, row in comparison_df.iterrows():
            axes[1, 1].annotate(row['Model'].split()[-1], 
                              (row['Precision'], row['Recall']),
                              fontsize=9, alpha=0.7)
        axes[1, 1].set_xlabel('Precision')
        axes[1, 1].set_ylabel('Recall')
        axes[1, 1].set_title('Precision-Recall Trade-off')
        axes[1, 1].set_xlim(0, 1)
        axes[1, 1].set_ylim(0, 1)
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(exp_dir / "results" / "model_comparison_chart.png", dpi=150, bbox_inches='tight')
        plt.close()